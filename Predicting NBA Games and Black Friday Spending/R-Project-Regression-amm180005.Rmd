---
title: "R Project - Regression"
subtitle: "4375 Machine Learning with Dr. Mazidi"
author: "Anthony Martinez | netid: amm180005"
date: "10/17/21"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
Project: Predict how much a person will spend on Black Friday using data set from Kaggle.

The data set was downloaded from Kaggle: https://www.kaggle.com/sdolezel/black-friday?select=train.csv

## Load the data

```{r}
df1<-read.csv("data/blackfriday.csv", header=TRUE)

```

## Data Cleaning

* Link to data: https://www.kaggle.com/sdolezel/black-friday?select=train.csv
* describe what steps you had to do for data cleaning (more points for messier
data that needed cleaning)

I Performed the following steps. 1) count NAs in all columns 2)  Deleted unneeded (All NAs ended up being in columns that were not needed). 3) Made all qualitative data into factors 

```{r}
sapply(df1, function(x) sum(is.na(x)))
# Here we see that product category2 and category 3 have a lot of NAs
# This is because category 2 and 3 are the "other" categories that a product could
# be included in. We will not be using these features as a predictor so I will 
# remove these columns from the data frame

df <- df1[-c(7,10,11)]

# output data frame with removed columns
sapply(df, function(x) sum(is.na(x)))

# make save factor columns, as factors
df$Gender <- factor(df$Gender, levels=c("M", "F"))
contrasts(df$Gender)

# from the unique function it can be seen that there are 7 categories for age
unique(df$Age)
# make age factor since the data does not report the person's age but rather their age range
df$Age <- factor(df$Age, levels=c("0-17", "55+","26-35","46-50","51-55","36-45","18-25"))
#contrasts(df$Age)

unique(df$Occupation)
df$Occupation <- factor(df$Occupation,levels=c(10,16,15,7,20,9,1,12,17,0,3,4,11,8,19,2, 18,5,14,13,6))

#contrasts(df$Occupation)


unique(df$City_Category)
df$City_Category <- factor(df$City_Category, levels=c("A","B","C"))
#contrasts(df$City_Category)

# marital status 
df$Marital_Status <- factor(df$Marital_Status, levels=c("1","0"))
#contrasts(df$Marital_Status)

# prod category
unique(df$Product_Category_1)
df$Product_Category_1 <- factor(df$Product_Category_1, levels=c(3,1, 12,8,5,4,2,6,14,11,13,15,7,16,18,10,17,9,20,19))
#contrasts(df$Product_Category_1)
```
## Step 2 Data Exploreation

* use at least 5 R functions for data exploration
* create at least 2 informative R graphs for data exploration

```{r}
summary(df)
head(df)
colnames(df)
str(df)
# Our target variable is Purchase, which is how much money the observation (a person) spent on black Friday
mean(df$Purchase)

# Graphs
plot(df$Purchase~df$Occupation, xlab="Occupation", ylab="Purchase in $", main="Purchase and Occupation")

plot(df$Purchase~df$Product_Category_1, xlab="Product Category", ylab="Purchase", main="Purchase and Product Category")
```

## Divide train/test
* Divide into 75/25 train/test, using seed 1234

```{r}
set.seed(1234)
i <- sample(1:nrow(df), .75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]

```

## Algorithm 1: Linear Regression (Multiple LR)
* code to run the algorithms
* commentary on feature selection you selected and why
* code to compute your metrics for evaluation as well as commentary discussing the results

Commentary on Features Chosen:
I decided to use Age, Occupation, and City Category predictors
because they seem to be a good indication of the amount of disposable income
a person has available. The more disposable income they have, the more money
they are able to spend on black Friday.
I decided to use Product category as a predictor because the price of items 
is highly associated with its category. For example, tech/electronics will
cost more than clothing.

Commentary on Results: The linear regression model did a pretty good job at predicting
the target. The accuracy of 79% and mse of 9,129,462 which will be useful 
when comparing models. The R^2 value is .6 which is decent

```{r}
lm1 <- lm(Purchase~Age+Occupation+City_Category+Product_Category_1, data=train)
summary(lm1)
pred <- predict(lm1, newdata = test)
cor(pred, test$Purchase)
mse <- mean((pred-test$Purchase)^2)
mse
rmse <- sqrt(mse)
rmse

```

## Algorithm 2: Decision Tree
* code to run the algorithms
* commentary on feature selection you selected and why
* code to compute your metrics for evaluation as well as commentary discussing the results

Commentary of feature selected: 
I decided to use Age, Occupation, and City Category predictors
because they seem to be a good indication of the amount of disposable income
a person has available. The more disposable income they have, the more money
they are able to spend on black Friday.

Commentary on Results: The decision tree had an accuracy of 78.9 percent which
is very close to the multiple linear regression model. The mse of the decision 
tree is slightly higher than at 9,524,735. This means the decision tree did slightly worse overall
since it was not able to minimize the errors as well as the multiple linear regression model.
```{r}
library(tree)
tree1 <- tree(Purchase~Age+Occupation+City_Category+Product_Category_1, data=train)
summary(tree1)

pred_tree <- predict(tree1,newdata = test)
cor_tree <- cor(pred_tree, test$Purchase)
print(paste("Correlation:", cor_tree))
mse_tree <- mean((pred_tree-test$Purchase)^2)
mse_tree
rmse_tree <- sqrt(mean((pred_tree-test$Purchase)^2))
print(paste("rmse:",rmse_tree))

plot(tree1)
text(tree1, cex=.5, pretty = 0)
```


Algorithm 3: Simple Linear Regression
* code to run the algorithms
* commentary on feature selection you selected and why
* code to compute your metrics for evaluation as well as commentary discussing the results

Commentary on Features: For this simple linear regression model I decided to predict on Occupation. My thought process was that the salary amount
alone is enough to make accurate predictions on how much a person would spend on black friday. 

Commentary on Results: Looking at the results we see that my thought process was incorrect. The model had a very low accuracy at .05% and 
a very large mse of 25,183,647. The model had terrible results which means that Occupation is a poor predictor for the target value which is Purchase amount. 
```{r}
lm2 <- lm(Purchase~Occupation, data=train)
summary(lm2)
pred_m <- predict(lm2, newdata = test)
cor(pred_m, test$Purchase)
mse2 <- mean((pred_m-test$Purchase)^2)
mse2
rmse2 <- sqrt(mse2)
rmse2

```
## Step 8 Results analysis
* rank the algorithms from best to worst performing on your data
* add commentary on the performance of the algorithms
* your analysis concerning why the best performing algorithm worked best on that data
* commentary on what your script was able to learn from the data (big picture)
and if this is likely to be useful

Rank:
1. Multiple Linear Regression
2. Decision Tree
3. Simple Linear Regression

Commentary on the performance: I have most of my commentary of the algorithms above. The reason why I ranked Multiple Linear regression the highest is because it had an accuracy slightly higher than the Decision tree and had a slightly lower mse. The lower mse score means it was able to minimize errors better which is desirable. The simple linear Regression had the worst scores by far. The accuracy was less than 1% and an enormous mse score. 

Commentary on Best Preforming Algorithm: The best preforming algorithm was the Multiple Linear Regression model. This is because multiple LR is able to factor in several relationships that are somewhat correlated with the target variable to make predictions on future data. The model I created used enough variables to learn from the data to make accurate predictions.  

Big Picture: The big picture takeaway from running these models on this data set is that product category is the best predictor on the amount of money 
a person will spend on Black Friday. This can be found by running a simple linear regression model on Purchase~Product_Category which will result in an accuracy rate of 78%. Knowing this information companies can focus on advertising their Black Friday sales based on product category rather than other factors. Since we are not given the names of the product category and are only able to see the category number, we can't say exactly which product category induces the most spending from individuals.














